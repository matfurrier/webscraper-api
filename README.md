[EN](#english) | [PT](#portugues)

---

<a name="english"></a>
## ğŸ‡ºğŸ‡¸ English

# WebScraper API ğŸ•·ï¸ğŸ“Š
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)

RESTful API for periodic scraping with Playwright + FastAPI + MongoDB

A modern RESTful API built with FastAPI, capable of collecting web data using Playwright (JavaScript headless rendering), storing it in MongoDB, and displaying it via a dashboard or exportable endpoints (JSON/CSV). Ideal for automated scraping of public portals and generating periodic reports.

## âš™ï¸ Technologies

*   Python 3.10+
*   FastAPI
*   MongoDB
*   APScheduler
*   Playwright (for scraping, replacing the need for Selenium)
*   BeautifulSoup4 (for parsing HTML post-Playwright)
*   Pandas
*   Jinja2 (for Dashboard)
*   Docker & Docker Compose
*   Uvicorn & Gunicorn

## ğŸ“Œ Features

*   **Web Scraping:**
    *   Collects data from the [CEPEA/ESALQ Soybean Indicator](https://www.cepea.esalq.usp.br/br/indicador/soja.aspx) (configurable via `TARGET_URL`) using Playwright to render JavaScript.
    *   Runs periodically (configurable via `SCRAPE_INTERVAL_HOURS`).
*   **REST API (FastAPI) & UI:**
    *   `GET /`: Root path.
    *   `GET /dashboard`: Simple HTML dashboard (embedded frontend UI) to view the data.
    *   `GET /api/data`: Returns all collected data.
    *   `GET /api/data/{id}`: Returns a specific item by ID.
    *   `POST /api/scrape`: Triggers a manual scraping process (runs in the background).
    *   `GET /api/export/json`: Downloads all data in JSON format.
    *   `GET /api/export/csv`: Downloads all data in CSV format.
    *   Interactive API documentation (Swagger UI) available at `/docs`.
*   **Database:**
    *   MongoDB for data storage.
    *   Schema: `{ "_id": ObjectId (string in API), "fonte": "string", "dados": {"tabela_completa": List[List[str]] }, "data_coleta": "datetime" }` (The `dados` field now contains the full extracted table).
*   **Scheduling:**
    *   Scraping job runs at a defined interval (default: 12 hours).
*   **Docker:**
    *   Container for the FastAPI application and scheduler (includes Playwright setup).
    *   Container for MongoDB.
    *   Persistent volume for MongoDB data.

## ğŸ“ Folder Structure

```
webscraper-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py          # REST API routes (/api/*)
â”‚   â”œâ”€â”€ db.py           # MongoDB connection and Pydantic schemas
â”‚   â”œâ”€â”€ export.py       # Export functionalities (CSV/JSON)
â”‚   â”œâ”€â”€ main.py         # FastAPI app, UI routes, lifecycle
â”‚   â”œâ”€â”€ scheduler.py    # Scraping scheduler (APScheduler)
â”‚   â”œâ”€â”€ scraper.py      # Scraping logic (Playwright + BeautifulSoup)
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ dashboard.html  # Dashboard HTML via Jinja2
â”œâ”€â”€ Dockerfile          # Defines the Docker image for the application
â”œâ”€â”€ docker-compose.yml  # Orchestrates containers (API + MongoDB)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ .env.example        # Environment settings (example)
```

## ğŸš€ Running the Project

### Prerequisites

*   Docker
*   Docker Compose

### 1. Environment Variables

Copy the `.env.example` file to `.env` in the project root (`webscraper-api/`) and adjust the variables as needed:

```bash
cp .env.example .env
```

Contents of `.env` (example):

```env
SCRAPE_INTERVAL_HOURS=12
TARGET_URL=https://www.cepea.esalq.usp.br/br/indicador/soja.aspx
MONGO_URI=mongodb://mongodb:27017/
MONGO_DB_NAME=scraper_db
PORT=8000
```

**Note:** The `docker-compose.yml` already defines default values for these variables. The local `.env` file allows you to easily override them.

### 2. Build and Run with Docker Compose

In the project root (`webscraper-api/`), run:

```bash
docker compose up --build
```

This will build the application image (including Playwright dependency installation) and start the application and MongoDB containers.

The API will be accessible at `http://localhost:8000`.
The Dashboard will be at `http://localhost:8000/dashboard`.
The interactive documentation (Swagger UI) will be at `http://localhost:8000/docs`.

### 3. Accessing Endpoints

*   **Dashboard:** `GET http://localhost:8000/dashboard`
*   **List all data (API):** `GET http://localhost:8000/api/data`
*   **Fetch by ID (replace `{item_id}`):** `GET http://localhost:8000/api/data/{item_id}`
*   **Trigger manual scraping:** `POST http://localhost:8000/api/scrape`
*   **Export to JSON:** `GET http://localhost:8000/api/export/json`
*   **Export to CSV:** `GET http://localhost:8000/api/export/csv`

### 4. Stopping the Application

To stop the containers:

```bash
docker compose down
```

To stop and remove volumes (including MongoDB data):

```bash
docker compose down -v
```

## ğŸ”§ Local Development (Without Docker, requires Python and MongoDB)

1.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
2.  **Install Playwright dependencies:**
    ```bash
    playwright install --with-deps chromium
    ```
3.  **Set up MongoDB:** Ensure a MongoDB instance is running and accessible.
4.  **Environment Variables:** Create a `.env` file in the project root (`webscraper-api/`) with your settings (especially `MONGO_URI` pointing to your local instance, e.g., `mongodb://localhost:27017/`).
5.  **Run the application:**
    ```bash
    python app/main.py
    ```
    Or using Uvicorn directly for auto-reloading:
    ```bash
    uvicorn app.main:app --reload
    ```

---

<a name="portugues"></a>
## ğŸ‡§ğŸ‡· PortuguÃªs

# WebScraper API ğŸ•·ï¸ğŸ“Š
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)

API RESTful para scraping periÃ³dico com Playwright + FastAPI + MongoDB

Uma API RESTful moderna construÃ­da com FastAPI, capaz de coletar dados web com Playwright (JavaScript headless rendering), armazenar em MongoDB e exibir via dashboard ou endpoints exportÃ¡veis (JSON/CSV). Ideal para scraping automatizado de portais pÃºblicos e geraÃ§Ã£o de relatÃ³rios periÃ³dicos.

## âš™ï¸ Tecnologias

*   Python 3.10+
*   FastAPI
*   MongoDB
*   APScheduler
*   Playwright (para scraping, substituindo a necessidade de Selenium)
*   BeautifulSoup4 (para parsing HTML pÃ³s-Playwright)
*   Pandas
*   Jinja2 (para o Dashboard)
*   Docker & Docker Compose
*   Uvicorn & Gunicorn

## ğŸ“Œ Funcionalidades

*   **Scraping Web:**
    *   Coleta dados do [Indicador da Soja CEPEA/ESALQ](https://www.cepea.esalq.usp.br/br/indicador/soja.aspx) (configurÃ¡vel via `TARGET_URL`) usando Playwright para renderizar JavaScript.
    *   Roda periodicamente (configurÃ¡vel via `SCRAPE_INTERVAL_HOURS`).
*   **API REST (FastAPI) e UI:**
    *   `GET /`: PÃ¡gina inicial.
    *   `GET /dashboard`: Dashboard HTML simples (UI frontend embutido) para visualizaÃ§Ã£o dos dados.
    *   `GET /api/data`: Retorna todos os dados coletados.
    *   `GET /api/data/{id}`: Retorna um item especÃ­fico pelo ID.
    *   `POST /api/scrape`: Dispara um processo de scraping manualmente (roda em background).
    *   `GET /api/export/json`: Baixa todos os dados em formato JSON.
    *   `GET /api/export/csv`: Baixa todos os dados em formato CSV.
    *   DocumentaÃ§Ã£o interativa da API (Swagger UI) disponÃ­vel em `/docs`.
*   **Banco de Dados:**
    *   MongoDB para armazenamento dos dados.
    *   Schema: `{ "_id": ObjectId (string na API), "fonte": "string", "dados": {"tabela_completa": List[List[str]] }, "data_coleta": "datetime" }` (O campo `dados` agora contÃ©m a tabela completa extraÃ­da).
*   **Agendamento:**
    *   Job de scraping roda no intervalo definido (padrÃ£o: 12 horas).
*   **Docker:**
    *   ContÃªiner para a aplicaÃ§Ã£o FastAPI e o scheduler (inclui instalaÃ§Ã£o do Playwright).
    *   ContÃªiner para o MongoDB.
    *   Volume persistente para os dados do MongoDB.

## ğŸ“ Estrutura de Pastas

```
webscraper-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py          # Rotas da API REST (/api/*)
â”‚   â”œâ”€â”€ db.py           # ConexÃ£o com MongoDB e schemas Pydantic
â”‚   â”œâ”€â”€ export.py       # Funcionalidades de exportaÃ§Ã£o (CSV/JSON)
â”‚   â”œâ”€â”€ main.py         # AplicaÃ§Ã£o FastAPI, rotas UI, ciclo de vida
â”‚   â”œâ”€â”€ scheduler.py    # Agendador de scraping (APScheduler)
â”‚   â”œâ”€â”€ scraper.py      # LÃ³gica de scraping (Playwright + BeautifulSoup)
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ dashboard.html  # Dashboard HTML via Jinja2
â”œâ”€â”€ Dockerfile          # Define a imagem Docker para a aplicaÃ§Ã£o
â”œâ”€â”€ docker-compose.yml  # Orquestra containers (API + MongoDB)
â”œâ”€â”€ requirements.txt    # DependÃªncias Python
â””â”€â”€ .env.example        # ConfiguraÃ§Ãµes de ambiente (exemplo)
```

## ğŸš€ Rodando o Projeto

### PrÃ©-requisitos

*   Docker
*   Docker Compose

### 1. VariÃ¡veis de Ambiente

Copie o arquivo `.env.example` para `.env` na raiz do projeto (`webscraper-api/`) e ajuste as variÃ¡veis conforme necessÃ¡rio:

```bash
cp .env.example .env
```

ConteÃºdo do `.env` (exemplo):

```env
SCRAPE_INTERVAL_HOURS=12
TARGET_URL=https://www.cepea.esalq.usp.br/br/indicador/soja.aspx
MONGO_URI=mongodb://mongodb:27017/
MONGO_DB_NAME=scraper_db
PORT=8000
```

**Nota:** O `docker-compose.yml` jÃ¡ define valores padrÃ£o para essas variÃ¡veis. O arquivo `.env` local permite que vocÃª os sobrescreva facilmente.

### 2. Build e Run com Docker Compose

Na raiz do projeto (`webscraper-api/`), execute:

```bash
docker compose up --build
```

Isso irÃ¡ construir a imagem da aplicaÃ§Ã£o (incluindo a instalaÃ§Ã£o das dependÃªncias do Playwright), iniciar os contÃªineres da aplicaÃ§Ã£o e do MongoDB.

A API estarÃ¡ acessÃ­vel em `http://localhost:8000`.
O Dashboard estarÃ¡ em `http://localhost:8000/dashboard`.
A documentaÃ§Ã£o interativa (Swagger UI) estarÃ¡ em `http://localhost:8000/docs`.

### 3. Acessando os Endpoints

*   **Dashboard:** `GET http://localhost:8000/dashboard`
*   **Listar todos os dados (API):** `GET http://localhost:8000/api/data`
*   **Buscar por ID (substitua `{item_id}`):** `GET http://localhost:8000/api/data/{item_id}`
*   **Disparar scraping manual:** `POST http://localhost:8000/api/scrape`
*   **Exportar para JSON:** `GET http://localhost:8000/api/export/json`
*   **Exportar para CSV:** `GET http://localhost:8000/api/export/csv`

### 4. Parando a AplicaÃ§Ã£o

Para parar os contÃªineres:

```bash
docker compose down
```

Para parar e remover os volumes (incluindo dados do MongoDB):

```bash
docker compose down -v
```

## ğŸ”§ Desenvolvimento Local (Sem Docker, requer Python e MongoDB)

1.  **Instale as dependÃªncias Python:**
    ```bash
    pip install -r requirements.txt
    ```
2.  **Instale as dependÃªncias do Playwright:**
    ```bash
    playwright install --with-deps chromium
    ```
3.  **Configure o MongoDB:** Certifique-se de que uma instÃ¢ncia do MongoDB esteja rodando e acessÃ­vel.
4.  **VariÃ¡veis de Ambiente:** Crie um arquivo `.env` na raiz do projeto (`webscraper-api/`) com as configuraÃ§Ãµes (principalmente `MONGO_URI` apontando para sua instÃ¢ncia local, e.g., `mongodb://localhost:27017/`).
5.  **Rode a aplicaÃ§Ã£o:**
    ```bash
    python app/main.py
    ```
    Ou usando Uvicorn diretamente para recarregamento automÃ¡tico:
    ```bash
    uvicorn app.main:app --reload
    ``` 